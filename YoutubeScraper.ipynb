{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System setup \n",
    "\n",
    "Before we start, make sure to install the required libraries\n",
    "    \n",
    "    pip install bs4\n",
    "    pip install selenium\n",
    "\n",
    "\n",
    "e.g. For Chrome, download the appropriate webdriver from here: http://chromedriver.chromium.org/downloads, unzip it and save in current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import re \n",
    "import urllib\n",
    "import time\n",
    "import html\n",
    "import string\n",
    "\n",
    "\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a webdriver object and set options for headless browsing\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome('/Users/macuser/Downloads/chromedriver', options=options)#Need to change this to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this function for when scraping the channels the person is subscribed to\n",
    "def get_subs_soup(url,driver):\n",
    "    driver.get(url)\n",
    "    SCROLL_PAUSE_TIME = 10\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "       # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight);\")\n",
    "        \n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        #print(\"new\" , new_height)\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "        \n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') #beautiful soup object to be used for parsing html content\n",
    "        \n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "#Use this function for normal scraping\n",
    "def get_soup(url,driver):\n",
    "    driver.get(url)\n",
    "    res_html = driver.execute_script('return document.body.innerHTML')\n",
    "    soup = BeautifulSoup(res_html,'html.parser') \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extracts all channels that the person is subscribed to\n",
    "def scrape_subs_page(dir_url,driver):\n",
    "    print ('-'*20,'Scraping subscriptions page','-'*20)\n",
    "    channel_links = []\n",
    "    base_url = 'https://www.youtube.com'\n",
    "    channel_names = []\n",
    "    \n",
    "            \n",
    "    #execute js on webpage to load channel listings on webpage and get ready to parse the loaded HTML \n",
    "    soup = get_soup(dir_url,driver)  \n",
    "    \n",
    "    \n",
    "    new_link = soup.find_all('ytd-grid-channel-renderer', class_ = 'style-scope ytd-grid-renderer') #get list of all channels\n",
    "    \n",
    "    #Get list of all channel links\n",
    "    for link_holder in new_link: \n",
    "        rel_link = link_holder.find('a')['href']  \n",
    "        channel_links.append(base_url+rel_link)\n",
    "    \n",
    "    \n",
    "    #Get list of all channel names\n",
    "    link = soup.find_all('span', class_ = 'style-scope ytd-grid-channel-renderer')\n",
    "    for i in range(0,int(len(link)/2)):\n",
    "        channel_name = link[2*i].text\n",
    "        channel_names.append(channel_name)\n",
    "        \n",
    "            \n",
    "    print ('-'*20,'Found {} channels urls'.format(len(channel_links)),'-'*20)\n",
    "    return channel_links, channel_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For a particular channel collects the links for the last 30 videos\n",
    "def get_all_video_links(channel_url,driver):\n",
    "    \n",
    "    #load the HTML for each subscription channel\n",
    "    soup_1 = get_soup(channel_url,driver)\n",
    "    \n",
    "    #Click on the videos to show the list of all the videos for the channel\n",
    "    driver.find_element_by_xpath('//*[@id=\"tabsContent\"]/tp-yt-paper-tab[2]').click()\n",
    "    video_html = driver.current_url\n",
    "    \n",
    "    #load the HTML for the videos page\n",
    "    soup_2 = get_soup(video_html, driver)\n",
    "    \n",
    "    videos_links = []\n",
    "    video_titles = []\n",
    "    videos_url = []\n",
    "    \n",
    "    new_link = soup_2.find_all('ytd-grid-video-renderer', class_ = 'style-scope ytd-grid-renderer')  #get list of all videos\n",
    "    \n",
    "    base_url = 'https://www.youtube.com'\n",
    "    #get the links for the last recent 30 videos\n",
    "    for link_holder in new_link: \n",
    "        rel_link = link_holder.find('a')['href']  \n",
    "        video_titles.append(driver.find_element_by_xpath('//*[@id=\"video-title\"]').text)\n",
    "        videos_links.append(video_html+rel_link)\n",
    "        videos_url.append(base_url+rel_link)\n",
    "    \n",
    "    print ('Found {} videos'.format(len(videos_links)))\n",
    "    return videos_links, video_titles, videos_url\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets the number of videos that were posted in the last month for a particular channel\n",
    "def get_dates_scrape(video_url, driver):\n",
    "    #load the HTML file for the channel page\n",
    "    soup = get_soup(video_url,driver)\n",
    "    \n",
    "    \n",
    "    dates = []\n",
    "    \n",
    "    #get the list of the date posted for each video\n",
    "    link = soup.find_all('span', class_ = 'style-scope ytd-grid-video-renderer')\n",
    "    for i in range(0,int(len(link)/2)):\n",
    "        date = link[2*i+1].text\n",
    "        dates.append(date)\n",
    "        \n",
    "    count = 0\n",
    "    #videos that were posted in the last 24 hours\n",
    "    for j in range(25):\n",
    "        for i in range(len(dates)):\n",
    "            date = dates[i]\n",
    "            string = str(j) + ' hours ago'\n",
    "            if (date == string):\n",
    "                count = count+1\n",
    "                \n",
    "    #videos that were posted in the last day  \n",
    "    for i in range(len(dates)):\n",
    "        date = dates[i]\n",
    "        string = '1 day ago'\n",
    "        if (date == string):\n",
    "            count = count+1\n",
    "    \n",
    "    #videos that were posted in the last month\n",
    "    for j in range(31):\n",
    "        for i in range(len(dates)):\n",
    "            date = dates[i]\n",
    "            string = str(j) + ' days ago'\n",
    "            if (date == string):\n",
    "                count = count+1\n",
    "                \n",
    "    #videos that were posted in the last 4 weeks            \n",
    "    for k in range(5):\n",
    "        for i in range(len(dates)):\n",
    "            date = dates[i]\n",
    "            string = str(k) + ' weeks ago'\n",
    "            if (date == string):\n",
    "                count = count+1\n",
    "                \n",
    "            \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Gets the title and description of each video\n",
    "def get_stuff(url, driver):\n",
    "    #open each video that is valid\n",
    "    #driver.find_element_by_xpath('//*[@id=\"items\"]/ytd-grid-video-renderer['+str(n+1)+']').click()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    #get title\n",
    "    v_title = wait.until(EC.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR,\"h1.title yt-formatted-string\"))).text\n",
    "    #get description\n",
    "    v_description =  wait.until(EC.presence_of_element_located(\n",
    "            (By.CSS_SELECTOR,\"div#description yt-formatted-string\"))).text\n",
    "    \n",
    "    \n",
    "    \n",
    "    return v_title, v_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Find any deals or promotions from the descriptions\n",
    "def find_deal(desc):\n",
    "    \n",
    "    matched_lines = \"\"\n",
    "    \n",
    "    for line in desc.split(\"\\n\"):\n",
    "        if (\"%\" in line and \"off\" in line) or (\"$\" in line and \"off\" in line):\n",
    "            matched_lines = line.strip()\n",
    "        elif (\"%\" in line and \"OFF\" in line) or (\"$\" in line and \"OFF\" in line):\n",
    "            matched_lines = line.strip()\n",
    "        elif (\"sponsored\" in line) or (\"sponsoring\" in line):\n",
    "            matched_lines = line.strip()\n",
    "        elif (\"discount\" in line) or (\"DISCOUNT\" in line):\n",
    "            matched_lines = line.strip()\n",
    "        elif (\"giveaway\" in line) or (\"GIVEAWAY\" in line):\n",
    "            matched_lines = line.strip()\n",
    "    \n",
    "    return matched_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping subscriptions page --------------------\n",
      "-------------------- Found 13 channels urls --------------------\n",
      "ur mom ashley\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  5\n",
      "Any deals?: Yes\n",
      "Deals: now this is interesting....🤪 btw freebeat has a holiday deal for a limited time! Click here https://bit.ly/3FKWcLm & use CODE: ashleyyspecial for an extra $50 off on top of the existing deals on their website! Or on Amazon,  use my codes to get 5% off! from https://www.youtube.com/watch?v=laYMhcMJl80\n",
      "Any deals?: Yes\n",
      "Deals: are my roomies stylish or what 😎 BTW get an EXTRA 30% off your 1st thredUP order and FREE shipping w/ code: ASHLEY30 https://bit.ly/thredUPnov21_ASHLEY30 (Offer expires 12/31/21. Applies to US & Canada customers only. See site for full terms!) from https://www.youtube.com/watch?v=LnGyzNJ7IME\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "krist & yu\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  3\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: Go to https://go.magik.ly/ml/1d0ld/ and use code 'KRIST' for 10% off your order! ☺︎ from https://www.youtube.com/watch?v=Num4tzwZ0K4\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "GraceLeeMusic\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  0\n",
      "----------------------------------------\n",
      "mayuko\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  6\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: Thank you to Felix Gray for sponsoring this video and my eye health! https://bit.ly/30zvLZL Use code EVERYTHING15 for 15% off everything at checkout. Offer valid from 11/23 to 12/2. from https://www.youtube.com/watch?v=dQOJmz18vPQ\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: https://helixsleep.com/mayuko for up to $200 off your mattress and two free pillows! from https://www.youtube.com/watch?v=L4jCJh5_Lr8\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "Jubilee\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  13\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: BLACK FRIDAY SALE! Up to 40% off! Ends Monday 11/29 shop.jubileemedia.com from https://www.youtube.com/watch?v=wrQYHeodRio\n",
      "Any deals?: Yes\n",
      "Deals: BLACK FRIDAY SALE! This weekend get up to 40% off at Jubilee Shop! Sale from Friday 11/26 through Monday 11/29 - don't miss it! shop.jubileemedia.com from https://www.youtube.com/watch?v=41iTr_ija60\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "saranghoe\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  4\n",
      "Any deals?: Yes\n",
      "Deals: ✿Use code SARANGHOE to get 30% off your first month at Scentbird https://sbird.co/2Z28M9p from https://www.youtube.com/watch?v=lmn1aIIpLO8\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: With midterms and assignments piling up, succeed in school with tools like Grammarly! Sign up for a FREE account and get 20% off Grammarly Premium: https://grammarly.com/saranghoe from https://www.youtube.com/watch?v=KiCdjZmksTQ\n",
      "Any deals?: Yes\n",
      "Deals: use my code 15% off code: sarangahoe_yt from https://www.youtube.com/watch?v=eJ57h7BywAg\n",
      "----------------------------------------\n",
      "Keo Tsang\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  2\n",
      "Any deals?: Yes\n",
      "Deals: With finals and deadlines coming, make sure to clutch your essays with Grammarly! Sign up for a FREE account and get 20% off Grammarly Premium: https://grammarly.com/keo from https://www.youtube.com/watch?v=bCtF2UAQPqk\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "ashleybchoi\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  2\n",
      "Any deals?: Yes\n",
      "Deals: Get 15% off your first NEIWAI order with code: \"ASHLEY15\" from https://www.youtube.com/watch?v=G0jqKXx0HAs\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "AmandaRachLee\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  3\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "jedcal\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  2\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "Michelle Choi\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  3\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: For 25% off your first Aurate purchase, go to https://auratenewyork.com/michellec and use promo code michellec! from https://www.youtube.com/watch?v=oOh-_OLDTyM\n",
      "Any deals?: Yes\n",
      "Deals: Thanks to Ghirardelli for sponsoring this video. Be sure to visit and find them at your local grocery store! from https://www.youtube.com/watch?v=ZACSFysf1nE\n",
      "----------------------------------------\n",
      "Julie Cho 조윤서\n",
      "Found 20 videos\n",
      "Number of videos in the last month:  2\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "Wong Fu Productions\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  0\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#TEST SECTION 1\n",
    "\n",
    "dir_url = 'https://www.youtube.com/channel/UC2Gv-b5RwRR0Bk74-ll6f1Q/channels?view=56&shelf_id=0' #url for the subscriptions page\n",
    "channel_links, channel_names = scrape_subs_page(dir_url,driver)\n",
    "\n",
    "\n",
    "for i in range(len(channel_names)):\n",
    "    print (channel_names[i])\n",
    "    videos_links,video_titles, videos_url = get_all_video_links(channel_links[i],driver)\n",
    "    \n",
    "    n = get_dates_scrape(videos_links[i], driver)\n",
    "    print(\"Number of videos in the last month: \", n)\n",
    "    for j in range(n):\n",
    "        title, descr = get_stuff(videos_url[j],driver)\n",
    "        \n",
    "        deals = find_deal(descr)\n",
    "        \n",
    "        if (deals == \"\"):\n",
    "            print('Any deals?: No')\n",
    "        else:\n",
    "            print('Any deals?: Yes')\n",
    "            print('Deals:', deals, 'from', videos_url[j])\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Scraping subscriptions page --------------------\n",
      "-------------------- Found 6 channels urls --------------------\n",
      "Study To Success\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  3\n",
      "Any deals?: Yes\n",
      "Deals: - This giveaway is international and open to all countries! The shipping is free (completely covered by me) 🥰 However, if you’re from India, there’s a small chance you might have to pay an import tax depending on where you live. Over the past year I’ve sent prizes to nine Indians and two had to pay an import tax (apparently it wasn’t very high though). So please ask your parents if they would be okay with paying the tax if you end up having to before entering! This may apply to other countries I haven't shipped packages to yet, so be sure to double check your country's policies! from https://www.youtube.com/watch?v=nDFL37juBiY\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "Fatima Bah\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  9\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: USE MY CODE ‘GE4351’ FOR 15% OFF from https://www.youtube.com/watch?v=UmSaKgLgg1E\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: Use My Special Code to get EXTRA 20% OFF: OFF20 from https://www.youtube.com/watch?v=1BX8aAD0gdg\n",
      "Any deals?: Yes\n",
      "Deals: DYSON Supersonic Hairdryer GIVEAWAY: One Lucky Winner who purchases in November from https://www.youtube.com/watch?v=r7iFvhSmTHI\n",
      "Any deals?: No\n",
      "Any deals?: Yes\n",
      "Deals: Thank you IGK Hair x Sephora for sponsoring today's video! from https://www.youtube.com/watch?v=_O4AQ006vdM\n",
      "----------------------------------------\n",
      "KalynAbridged\n",
      "Found 30 videos\n",
      "Number of videos in the last month:  5\n",
      "Any deals?: Yes\n",
      "Deals: Thank you so much to Book of the Month for sponsoring this video!! from https://www.youtube.com/watch?v=5Wm5yezJnDU\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "Any deals?: No\n",
      "----------------------------------------\n",
      "Alexandra Roselyn\n"
     ]
    }
   ],
   "source": [
    "#TEST SECTION 2\n",
    "\n",
    "dir_url = 'https://www.youtube.com/channel/UCSm4whN-PJgZ4uzioQDlUCQ/channels?view=56&shelf_id=0' #url for the subscriptions page\n",
    "channel_links, channel_names = scrape_subs_page(dir_url,driver)\n",
    "\n",
    "\n",
    "for i in range(len(channel_names)):\n",
    "    print (channel_names[i])\n",
    "    videos_links,video_titles, videos_url = get_all_video_links(channel_links[i],driver)\n",
    "    \n",
    "    n = get_dates_scrape(videos_links[i], driver)\n",
    "    print(\"Number of videos in the last month: \", n)\n",
    "    for j in range(n):\n",
    "        title, descr = get_stuff(videos_url[j],driver)\n",
    "        \n",
    "        deals = find_deal(descr)\n",
    "        \n",
    "        if (deals == \"\"):\n",
    "            print('Any deals?: No')\n",
    "        else:\n",
    "            print('Any deals?: Yes')\n",
    "            print('Deals:', deals, 'from', videos_url[j])\n",
    "    print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
